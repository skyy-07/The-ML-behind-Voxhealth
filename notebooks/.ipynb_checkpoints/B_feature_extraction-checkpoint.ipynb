{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook B: HeAR Feature Extraction\n",
    "## Generate embeddings using HeAR model\n",
    "\n",
    "Loads the HeAR model from Hugging Face using direct TensorFlow SavedModel loading to ensure compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: huggingface_hub in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (0.36.2)\n",
      "Requirement already satisfied: librosa in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (0.11.0)\n",
      "Requirement already satisfied: tensorflow in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (2.20.0)\n",
      "Requirement already satisfied: filelock in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from huggingface_hub) (3.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from huggingface_hub) (2025.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from huggingface_hub) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from huggingface_hub) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from huggingface_hub) (4.14.1)\n",
      "Requirement already satisfied: audioread>=2.1.9 in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from librosa) (3.1.0)\n",
      "Requirement already satisfied: numba>=0.51.0 in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from librosa) (0.63.1)\n",
      "Requirement already satisfied: numpy>=1.22.3 in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from librosa) (2.3.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from librosa) (1.16.2)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from librosa) (1.7.2)\n",
      "Requirement already satisfied: joblib>=1.0 in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from librosa) (1.5.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from librosa) (5.2.1)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from librosa) (0.13.1)\n",
      "Requirement already satisfied: pooch>=1.1 in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from librosa) (1.9.0)\n",
      "Requirement already satisfied: soxr>=0.3.2 in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from librosa) (1.0.0)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from librosa) (1.1.2)\n",
      "Requirement already satisfied: standard-aifc in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from librosa) (3.13.0)\n",
      "Requirement already satisfied: standard-sunau in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from librosa) (3.13.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from tensorflow) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from tensorflow) (6.32.1)\n",
      "Requirement already satisfied: setuptools in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from tensorflow) (80.9.0)\n",
      "Requirement already satisfied: six>=1.12.0 in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from tensorflow) (1.17.3)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from tensorflow) (1.75.0)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from tensorflow) (2.20.0)\n",
      "Requirement already satisfied: keras>=3.10.0 in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from tensorflow) (3.11.3)\n",
      "Requirement already satisfied: h5py>=3.11.0 in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from tensorflow) (3.14.0)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from tensorflow) (0.5.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from requests->huggingface_hub) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from requests->huggingface_hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from requests->huggingface_hub) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from requests->huggingface_hub) (2025.8.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.9)\n",
      "Requirement already satisfied: pillow in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from tensorboard~=2.20.0->tensorflow) (11.3.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from tensorboard~=2.20.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from keras>=3.10.0->tensorflow) (14.1.0)\n",
      "Requirement already satisfied: namex in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from keras>=3.10.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from keras>=3.10.0->tensorflow) (0.17.0)\n",
      "Requirement already satisfied: llvmlite<0.47,>=0.46.0dev0 in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from numba>=0.51.0->librosa) (0.46.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from pooch>=1.1->librosa) (4.4.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
      "Requirement already satisfied: cffi>=1.0 in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
      "Requirement already satisfied: pycparser in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
      "Requirement already satisfied: colorama in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from tqdm>=4.42.1->huggingface_hub) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from rich->keras>=3.10.0->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from rich->keras>=3.10.0->tensorflow) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.2)\n",
      "Requirement already satisfied: standard-chunk in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from standard-aifc->librosa) (3.13.0)\n",
      "Requirement already satisfied: audioop-lts in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from standard-aifc->librosa) (0.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Embeddings output: D:\\datasets\\embeddings\n",
      "TensorFlow version: 2.20.0\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "%pip install huggingface_hub librosa tensorflow\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "DATASETS_ROOT = Path(r\"D:\\datasets\")\n",
    "PROCESSED_ROOT = DATASETS_ROOT / 'processed'\n",
    "EMBEDDINGS_DIR = DATASETS_ROOT / 'embeddings'\n",
    "EMBEDDINGS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TARGET_SR = 16000\n",
    "N_SAMPLES = 32000  # 2 seconds at 16kHz\n",
    "EMBEDDING_DIM = 768 \n",
    "\n",
    "print(f\"Embeddings output: {EMBEDDINGS_DIR}\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading HeAR model from Hugging Face Hub...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54ebed9621a54f3189a31ad831a78563",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 24 files:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model path: C:\\Users\\PC\\.cache\\huggingface\\hub\\models--google--hear\\snapshots\\9b2eb2853c426676255cc6ac5804b7f1fe8e563f\n",
      "Loading SavedModel...\n",
      "⚠ Failed to load model: Importing a SavedModel with `tf.saved_model.load` requires a `tags=` argument if there is more than one MetaGraph. Got `tags=None`, but there are 0 MetaGraphs in the SavedModel with tag sets: []. Pass a `tags=` argument to load this SavedModel.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Importing a SavedModel with `tf.saved_model.load` requires a `tags=` argument if there is more than one MetaGraph. Got `tags=None`, but there are 0 MetaGraphs in the SavedModel with tag sets: []. Pass a `tags=` argument to load this SavedModel.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     14\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m⚠ Failed to load model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel path: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLoading SavedModel...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m hear_model = \u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43msaved_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m inference_fn = hear_model.signatures[\u001b[33m\"\u001b[39m\u001b[33mserving_default\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✓ Model loaded successfully using tf.saved_model.load\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\tensorflow\\python\\saved_model\\load.py:912\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(export_dir, tags, options)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(export_dir, os.PathLike):\n\u001b[32m    911\u001b[39m   export_dir = os.fspath(export_dir)\n\u001b[32m--> \u001b[39m\u001b[32m912\u001b[39m result = \u001b[43mload_partial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexport_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mroot\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    913\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\tensorflow\\python\\saved_model\\load.py:1070\u001b[39m, in \u001b[36mload_partial\u001b[39m\u001b[34m(export_dir, filters, tags, options)\u001b[39m\n\u001b[32m   1067\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mSavedModels saved from Tensorflow 1.x) cannot be \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1068\u001b[39m                      \u001b[33m\"\u001b[39m\u001b[33mloaded with node filters.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1069\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m ops.init_scope():\n\u001b[32m-> \u001b[39m\u001b[32m1070\u001b[39m     root = \u001b[43mload_v1_in_v2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1071\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexport_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexperimental_skip_checkpoint\u001b[49m\n\u001b[32m   1072\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1073\u001b[39m     root.graph_debug_info = debug_info\n\u001b[32m   1074\u001b[39m \u001b[38;5;66;03m# For privacy concerns, please see the note in\u001b[39;00m\n\u001b[32m   1075\u001b[39m \u001b[38;5;66;03m#  tensorflow/cc/saved_model/metrics.h\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\tensorflow\\python\\saved_model\\load_v1_in_v2.py:316\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(export_dir, tags, skip_restoring_checkpoint)\u001b[39m\n\u001b[32m    314\u001b[39m metrics.IncrementReadApi(_LOAD_V1_V2_LABEL)\n\u001b[32m    315\u001b[39m loader = _EagerSavedModelLoader(export_dir)\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m result = \u001b[43mloader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_restoring_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_restoring_checkpoint\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    319\u001b[39m metrics.IncrementRead(write_version=\u001b[33m\"\u001b[39m\u001b[33m1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    320\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\tensorflow\\python\\saved_model\\load_v1_in_v2.py:229\u001b[39m, in \u001b[36m_EagerSavedModelLoader.load\u001b[39m\u001b[34m(self, tags, skip_restoring_checkpoint)\u001b[39m\n\u001b[32m    227\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\u001b[38;5;28mself\u001b[39m, tags, skip_restoring_checkpoint=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    228\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Creates an object from the MetaGraph identified by `tags`.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m229\u001b[39m   meta_graph_def = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_meta_graph_def_from_tags\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    230\u001b[39m   load_shared_name_suffix = \u001b[33m\"\u001b[39m\u001b[33m_load_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(ops.uid())\n\u001b[32m    231\u001b[39m   functions = function_deserialization.load_function_def_library(\n\u001b[32m    232\u001b[39m       meta_graph_def.graph_def.library,\n\u001b[32m    233\u001b[39m       load_shared_name_suffix=load_shared_name_suffix,\n\u001b[32m    234\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\tensorflow\\python\\saved_model\\load_v1_in_v2.py:81\u001b[39m, in \u001b[36m_EagerSavedModelLoader.get_meta_graph_def_from_tags\u001b[39m\u001b[34m(self, tags)\u001b[39m\n\u001b[32m     77\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._saved_model.meta_graphs) != \u001b[32m1\u001b[39m:\n\u001b[32m     78\u001b[39m     tag_sets = [\n\u001b[32m     79\u001b[39m         mg.meta_info_def.tags \u001b[38;5;28;01mfor\u001b[39;00m mg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._saved_model.meta_graphs\n\u001b[32m     80\u001b[39m     ]\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     82\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mImporting a SavedModel with `tf.saved_model.load` requires a \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     83\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`tags=` argument if there is more than one MetaGraph. Got \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     84\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m`tags=None`, but there are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._saved_model.meta_graphs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     85\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMetaGraphs in the SavedModel with tag sets: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtag_sets\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Pass a \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     86\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`tags=` argument to load this SavedModel.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     87\u001b[39m     )\n\u001b[32m     88\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._saved_model.meta_graphs[\u001b[32m0\u001b[39m]\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(_EagerSavedModelLoader, \u001b[38;5;28mself\u001b[39m).get_meta_graph_def_from_tags(\n\u001b[32m     90\u001b[39m     tags\n\u001b[32m     91\u001b[39m )\n",
      "\u001b[31mValueError\u001b[39m: Importing a SavedModel with `tf.saved_model.load` requires a `tags=` argument if there is more than one MetaGraph. Got `tags=None`, but there are 0 MetaGraphs in the SavedModel with tag sets: []. Pass a `tags=` argument to load this SavedModel."
     ]
    }
   ],
   "source": [
    "# Load HeAR Model directly as a SavedModel\n",
    "print(\"Downloading HeAR model from Hugging Face Hub...\")\n",
    "\n",
    "try:\n",
    "    model_path = snapshot_download(repo_id=\"google/hear\", repo_type=\"model\")\n",
    "    print(f\"Model path: {model_path}\")\n",
    "    \n",
    "    print(\"Loading SavedModel...\")\n",
    "    hear_model = tf.saved_model.load(model_path)\n",
    "    inference_fn = hear_model.signatures[\"serving_default\"]\n",
    "    print(\"✓ Model loaded successfully using tf.saved_model.load\")\n",
    "    print(f\"Signatures: {list(hear_model.signatures.keys())}\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠ Failed to load model: {e}\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio_for_hear(file_path):\n",
    "    \"\"\"Load audio and ensure it's exactly 32000 samples (2s @ 16kHz)\"\"\"\n",
    "    # HeAR expects normalized audio in range [-1, 1]\n",
    "    audio, _ = librosa.load(str(file_path), sr=TARGET_SR, mono=True)\n",
    "    \n",
    "    # Pad or trim to exactly N_SAMPLES\n",
    "    if len(audio) < N_SAMPLES:\n",
    "        audio = np.pad(audio, (0, N_SAMPLES - len(audio)), 'constant')\n",
    "    else:\n",
    "        audio = audio[:N_SAMPLES]\n",
    "        \n",
    "    return audio.astype(np.float32)\n",
    "\n",
    "def extract_embeddings_batch(audio_batch):\n",
    "    \"\"\"Batch extraction using the serving_default signature\"\"\"\n",
    "    # Convert input list to tensor\n",
    "    audio_tensor = tf.convert_to_tensor(audio_batch)\n",
    "    \n",
    "    # Inference: The model expects input key 'x'\n",
    "    # Note: tf.saved_model signatures often return a dict\n",
    "    output_dict = inference_fn(x=audio_tensor)\n",
    "    \n",
    "    # The output key is usually 'output_0'\n",
    "    if 'output_0' in output_dict:\n",
    "        embedding = output_dict['output_0'].numpy()\n",
    "    else:\n",
    "        # Fallback if key is different (though 'output_0' is standard for HeAR)\n",
    "        key = list(output_dict.keys())[0]\n",
    "        embedding = output_dict[key].numpy()\n",
    "        \n",
    "    return embedding\n",
    "\n",
    "print(\"✓ Embedding functions ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset_embeddings(dataset_name, batch_size=32):\n",
    "    input_dir = PROCESSED_ROOT / dataset_name\n",
    "    if not input_dir.exists():\n",
    "        print(f\"⚠ {dataset_name}: Not found\")\n",
    "        return None\n",
    "\n",
    "    wav_files = sorted(input_dir.glob(\"*.wav\"))\n",
    "    if not wav_files:\n",
    "        return None\n",
    "\n",
    "    print(f\"\\nProcessing {dataset_name}: {len(wav_files)} files\")\n",
    "\n",
    "    embeddings_list = []\n",
    "    file_names = []\n",
    "\n",
    "    # Process in batches\n",
    "    for i in tqdm(range(0, len(wav_files), batch_size), desc=f\"Extracting {dataset_name}\"):\n",
    "        batch_files = wav_files[i:i+batch_size]\n",
    "        \n",
    "        # Load batch audio\n",
    "        batch_audio = []\n",
    "        valid_batch_indices = []\n",
    "        \n",
    "        for idx, f in enumerate(batch_files):\n",
    "            try:\n",
    "                audio = load_audio_for_hear(f)\n",
    "                batch_audio.append(audio)\n",
    "                valid_batch_indices.append(idx)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {f}: {e}\")\n",
    "        \n",
    "        if not batch_audio:\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            batch_embeddings = extract_embeddings_batch(batch_audio)\n",
    "            embeddings_list.append(batch_embeddings)\n",
    "            file_names.extend([batch_files[idx].stem for idx in valid_batch_indices])\n",
    "        except Exception as e:\n",
    "            print(f\"Error batch {i}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if not embeddings_list:\n",
    "        return 0\n",
    "\n",
    "    embeddings = np.vstack(embeddings_list)\n",
    "    output_path = EMBEDDINGS_DIR / f\"{dataset_name}_embeddings.npz\"\n",
    "    np.savez_compressed(output_path, embeddings=embeddings, file_names=file_names)\n",
    "    print(f\"✓ {dataset_name}: {embeddings.shape[0]} embeddings saved\")\n",
    "    return embeddings.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all datasets\n",
    "datasets = ['coughvid', 'parkinsons', 'respiratory_sounds', 'coswara']\n",
    "results = {}\n",
    "\n",
    "for name in datasets:\n",
    "    results[name] = process_dataset_embeddings(name)\n",
    "\n",
    "summary = {'embedding_dim': EMBEDDING_DIM, 'sample_rate': TARGET_SR, 'datasets': results}\n",
    "with open(EMBEDDINGS_DIR / 'embeddings_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"\\nTotal embeddings: {sum(v for v in results.values() if v)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
