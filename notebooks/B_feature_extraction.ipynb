{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Notebook B: Feature Extraction (Robust Multi-Strategy)\n",
                "\n",
                "## ⚠️ IMPORTANT\n",
                "**Ensure you are running `notebooks/B_feature_extraction.ipynb`, NOT a file in `.ipynb_checkpoints`.**\n",
                "\n",
                "## Overview\n",
                "This notebook generates audio embeddings for the classifiers. It employs a **Multi-Strategy Loading Mechanism** to handle environment compatibility issues:\n",
                "1.  **Strategy A**: Try loading **HeAR** (Health Acoustic Representations) using `transformers` (Recommended).\n",
                "2.  **Strategy B**: Try loading **HeAR** using native `tf.saved_model` or `hub.load` (Backup).\n",
                "3.  **Strategy C**: Fallback to **YAMNet** (Google's Audio Event Model) via TensorFlow Hub if HeAR fails.\n",
                "\n",
                "This ensures the pipeline, including Notebook C and Validation, can proceed even if the primary HeAR model faces compatibility issues with Python 3.13/TF 2.16."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Defaulting to user installation because normal site-packages is not writeable\n",
                        "Requirement already satisfied: transformers in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (4.46.0)\n",
                        "Requirement already satisfied: tensorflow_hub in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (0.16.1)\n",
                        "Requirement already satisfied: tf-keras in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (2.20.1)\n",
                        "Requirement already satisfied: filelock in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from transformers) (3.19.1)\n",
                        "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from transformers) (0.36.2)\n",
                        "Requirement already satisfied: numpy>=1.17 in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from transformers) (2.3.3)\n",
                        "Requirement already satisfied: packaging>=20.0 in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from transformers) (25.0)\n",
                        "Requirement already satisfied: pyyaml>=5.1 in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from transformers) (6.0.2)\n",
                        "Requirement already satisfied: regex!=2019.12.17 in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from transformers) (2025.9.1)\n",
                        "Requirement already satisfied: requests in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from transformers) (2.32.5)\n",
                        "Requirement already satisfied: safetensors>=0.4.1 in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from transformers) (0.7.0)\n",
                        "Requirement already satisfied: tokenizers<0.21,>=0.20 in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from transformers) (0.20.3)\n",
                        "Requirement already satisfied: tqdm>=4.27 in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from transformers) (4.67.1)\n",
                        "Requirement already satisfied: fsspec>=2023.5.0 in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2025.9.0)\n",
                        "Requirement already satisfied: typing-extensions>=3.7.4.3 in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.14.1)\n",
                        "Requirement already satisfied: protobuf>=3.19.6 in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from tensorflow_hub) (6.32.1)\n",
                        "Requirement already satisfied: tensorflow<2.21,>=2.20 in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from tf-keras) (2.20.0)\n",
                        "Requirement already satisfied: absl-py>=1.0.0 in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.3.1)\n",
                        "Requirement already satisfied: astunparse>=1.6.0 in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.6.3)\n",
                        "Requirement already satisfied: flatbuffers>=24.3.25 in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (25.2.10)\n",
                        "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (0.6.0)\n",
                        "Requirement already satisfied: google_pasta>=0.1.1 in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (0.2.0)\n",
                        "Requirement already satisfied: libclang>=13.0.0 in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (18.1.1)\n",
                        "Requirement already satisfied: opt_einsum>=2.3.2 in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (3.4.0)\n",
                        "Requirement already satisfied: setuptools in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (80.9.0)\n",
                        "Requirement already satisfied: six>=1.12.0 in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.17.0)\n",
                        "Requirement already satisfied: termcolor>=1.1.0 in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (3.1.0)\n",
                        "Requirement already satisfied: wrapt>=1.11.0 in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.17.3)\n",
                        "Requirement already satisfied: grpcio<2.0,>=1.24.3 in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.75.0)\n",
                        "Requirement already satisfied: tensorboard~=2.20.0 in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.20.0)\n",
                        "Requirement already satisfied: keras>=3.10.0 in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (3.11.3)\n",
                        "Requirement already satisfied: h5py>=3.11.0 in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (3.14.0)\n",
                        "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (0.5.3)\n",
                        "Requirement already satisfied: charset_normalizer<4,>=2 in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from requests->transformers) (3.4.3)\n",
                        "Requirement already satisfied: idna<4,>=2.5 in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from requests->transformers) (3.10)\n",
                        "Requirement already satisfied: urllib3<3,>=1.21.1 in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from requests->transformers) (2.5.0)\n",
                        "Requirement already satisfied: certifi>=2017.4.17 in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from requests->transformers) (2025.8.3)\n",
                        "Requirement already satisfied: markdown>=2.6.8 in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (3.9)\n",
                        "Requirement already satisfied: pillow in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (11.3.0)\n",
                        "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (0.7.2)\n",
                        "Requirement already satisfied: werkzeug>=1.0.1 in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (3.1.3)\n",
                        "Requirement already satisfied: wheel<1.0,>=0.23.0 in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from astunparse>=1.6.0->tensorflow<2.21,>=2.20->tf-keras) (0.45.1)\n",
                        "Requirement already satisfied: rich in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (14.1.0)\n",
                        "Requirement already satisfied: namex in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (0.1.0)\n",
                        "Requirement already satisfied: optree in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (0.17.0)\n",
                        "Requirement already satisfied: colorama in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
                        "Requirement already satisfied: MarkupSafe>=2.1.1 in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (3.0.2)\n",
                        "Requirement already satisfied: markdown-it-py>=2.2.0 in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from rich->keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (4.0.0)\n",
                        "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from rich->keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (2.19.2)\n",
                        "Requirement already satisfied: mdurl~=0.1 in C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (0.1.2)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages\\tensorflow_hub\\__init__.py:61: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
                        "  from pkg_resources import parse_version\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "WARNING:tensorflow:From C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
                        "\n",
                        "Embeddings output: D:\\datasets\\embeddings\n",
                        "TensorFlow version: 2.20.0\n"
                    ]
                }
            ],
            "source": [
                "# Install necessary packages including transformers and tensorflow_hub\n",
                "!pip install transformers tensorflow_hub tf-keras\n",
                "\n",
                "import os\n",
                "from pathlib import Path\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import librosa\n",
                "import tensorflow as tf\n",
                "import tensorflow_hub as hub\n",
                "from tqdm.notebook import tqdm\n",
                "import json\n",
                "import pickle\n",
                "\n",
                "DATASETS_ROOT = Path(r\"D:\\datasets\")\n",
                "PROCESSED_ROOT = DATASETS_ROOT / 'processed'\n",
                "EMBEDDINGS_DIR = DATASETS_ROOT / 'embeddings'\n",
                "EMBEDDINGS_DIR.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "TARGET_SR = 16000\n",
                "N_SAMPLES = 32000  # 2 seconds at 16kHz\n",
                "\n",
                "print(f\"Embeddings output: {EMBEDDINGS_DIR}\")\n",
                "print(f\"TensorFlow version: {tf.__version__}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "✓ Already logged in to Hugging Face\n"
                    ]
                }
            ],
            "source": [
                "# Login to Hugging Face (Required for HeAR)\n",
                "from huggingface_hub import notebook_login\n",
                "try:\n",
                "    from huggingface_hub import get_token\n",
                "except ImportError:\n",
                "    from huggingface_hub import HfFolder\n",
                "    get_token = HfFolder.get_token\n",
                "\n",
                "if get_token() is None:\n",
                "    print(\"Please login to Hugging Face to access HeAR model:\")\n",
                "    notebook_login()\n",
                "else:\n",
                "    print(\"✓ Already logged in to Hugging Face\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "--- Attempting Strategy A: HeAR via Transformers ---\n",
                        "❌ Failed Strategy A: Unrecognized model in google/hear. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: albert, align, altclip, audio-spectrogram-transformer, autoformer, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, blenderbot, blenderbot-small, blip, blip-2, bloom, bridgetower, bros, camembert, canine, chameleon, chinese_clip, chinese_clip_vision_model, clap, clip, clip_text_model, clip_vision_model, clipseg, clvp, code_llama, codegen, cohere, conditional_detr, convbert, convnext, convnextv2, cpmant, ctrl, cvt, dac, data2vec-audio, data2vec-text, data2vec-vision, dbrx, deberta, deberta-v2, decision_transformer, deformable_detr, deit, depth_anything, deta, detr, dinat, dinov2, distilbert, donut-swin, dpr, dpt, efficientformer, efficientnet, electra, encodec, encoder-decoder, ernie, ernie_m, esm, falcon, falcon_mamba, fastspeech2_conformer, flaubert, flava, fnet, focalnet, fsmt, funnel, fuyu, gemma, gemma2, git, glm, glpn, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, granite, granitemoe, graphormer, grounding-dino, groupvit, hiera, hubert, ibert, idefics, idefics2, idefics3, imagegpt, informer, instructblip, instructblipvideo, jamba, jetmoe, jukebox, kosmos-2, layoutlm, layoutlmv2, layoutlmv3, led, levit, lilt, llama, llava, llava_next, llava_next_video, llava_onevision, longformer, longt5, luke, lxmert, m2m_100, mamba, mamba2, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mimi, mistral, mixtral, mllama, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, moshi, mpnet, mpt, mra, mt5, musicgen, musicgen_melody, mvp, nat, nemotron, nezha, nllb-moe, nougat, nystromformer, olmo, olmoe, omdet-turbo, oneformer, open-llama, openai-gpt, opt, owlv2, owlvit, paligemma, patchtsmixer, patchtst, pegasus, pegasus_x, perceiver, persimmon, phi, phi3, phimoe, pix2struct, pixtral, plbart, poolformer, pop2piano, prophetnet, pvt, pvt_v2, qdqbert, qwen2, qwen2_audio, qwen2_audio_encoder, qwen2_moe, qwen2_vl, rag, realm, recurrent_gemma, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rt_detr, rt_detr_resnet, rwkv, sam, seamless_m4t, seamless_m4t_v2, segformer, seggpt, sew, sew-d, siglip, siglip_vision_model, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, stablelm, starcoder2, superpoint, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, table-transformer, tapas, time_series_transformer, timesformer, timm_backbone, trajectory_transformer, transfo-xl, trocr, tvlt, tvp, udop, umt5, unispeech, unispeech-sat, univnet, upernet, van, video_llava, videomae, vilt, vipllava, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, vitdet, vitmatte, vits, vivit, wav2vec2, wav2vec2-bert, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xmod, yolos, yoso, zamba, zoedepth\n",
                        "\n",
                        "--- Attempting Strategy B: HeAR via Native TF/Hub ---\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "2a9d4a4c3dae4ee68979ea712da1399d",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Fetching 24 files:   0%|          | 0/24 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "❌ Failed Strategy B: Importing a SavedModel with `tf.saved_model.load` requires a `tags=` argument if there is more than one MetaGraph. Got `tags=None`, but there are 0 MetaGraphs in the SavedModel with tag sets: []. Pass a `tags=` argument to load this SavedModel.\n",
                        "\n",
                        "--- Attempting Strategy C: YAMNet Fallback (TFHub) ---\n",
                        "WARNING:tensorflow:From C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages\\tensorflow_hub\\resolver.py:120: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
                        "\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "WARNING:tensorflow:From C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages\\tensorflow_hub\\resolver.py:120: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
                        "\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "WARNING:tensorflow:From C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages\\tensorflow_hub\\module_v2.py:126: The name tf.saved_model.load_v2 is deprecated. Please use tf.compat.v2.saved_model.load instead.\n",
                        "\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "WARNING:tensorflow:From C:\\Users\\PC\\AppData\\Roaming\\Python\\Python313\\site-packages\\tensorflow_hub\\module_v2.py:126: The name tf.saved_model.load_v2 is deprecated. Please use tf.compat.v2.saved_model.load instead.\n",
                        "\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "✅ Success: Loaded YAMNet explicitly.\n",
                        "⚠️ NOTE: Using YAMNet instead of HeAR due to environment compatibility issues.\n",
                        "   This allows you to proceed with the pipeline. You can switch back later.\n",
                        "\n",
                        "FINAL MODEL SELECTION: YAMNet (Dim: 1024)\n"
                    ]
                }
            ],
            "source": [
                "# --- MODEL LOADING SECTION ---\n",
                "model_name = None\n",
                "embedding_model = None\n",
                "EMBEDDING_DIM = 0\n",
                "\n",
                "# Strategy A: Transformers (HeAR)\n",
                "try:\n",
                "    print(\"\\n--- Attempting Strategy A: HeAR via Transformers ---\")\n",
                "    from transformers import TFAutoModel\n",
                "    embedding_model = TFAutoModel.from_pretrained(\"google/hear\", trust_remote_code=True)\n",
                "    model_name = \"HeAR_Transformers\"\n",
                "    EMBEDDING_DIM = 768  # HeAR base usually 768\n",
                "    print(\"✅ Success: Loaded HeAR via Transformers\")\n",
                "except Exception as e:\n",
                "    print(f\"❌ Failed Strategy A: {e}\")\n",
                "\n",
                "# Strategy B: Native TF (HeAR) - Only if A failed\n",
                "if model_name is None:\n",
                "    try:\n",
                "        print(\"\\n--- Attempting Strategy B: HeAR via Native TF/Hub ---\")\n",
                "        from huggingface_hub import snapshot_download\n",
                "        path = snapshot_download(\"google/hear\")\n",
                "        # Try loading sub-model first as it showed more promise\n",
                "        sub_path = os.path.join(path, \"event_detector\", \"event_detector_large\")\n",
                "        try:\n",
                "            # Try loading sub-model\n",
                "            embedding_model = tf.saved_model.load(sub_path, tags=['serve'])\n",
                "            model_name = \"HeAR_SubModel\"\n",
                "        except:\n",
                "            # Try root model\n",
                "             embedding_model = tf.saved_model.load(path)\n",
                "             model_name = \"HeAR_Root\"\n",
                "        \n",
                "        EMBEDDING_DIM = 768 # Assumption\n",
                "        print(f\"✅ Success: Loaded HeAR via Native TF ({model_name})\")\n",
                "    except Exception as e:\n",
                "        print(f\"❌ Failed Strategy B: {e}\")\n",
                "\n",
                "# Strategy C: YAMNet Fallback - If A and B failed\n",
                "if model_name is None:\n",
                "    try:\n",
                "        print(\"\\n--- Attempting Strategy C: YAMNet Fallback (TFHub) ---\")\n",
                "        # YAMNet is robust standard feature extractor\n",
                "        embedding_model = hub.load('https://tfhub.dev/google/yamnet/1')\n",
                "        model_name = \"YAMNet\"\n",
                "        EMBEDDING_DIM = 1024 # YAMNet dimensions\n",
                "        print(\"✅ Success: Loaded YAMNet explicitly.\")\n",
                "        print(\"⚠️ NOTE: Using YAMNet instead of HeAR due to environment compatibility issues.\")\n",
                "        print(\"   This allows you to proceed with the pipeline. You can switch back later.\")\n",
                "    except Exception as e:\n",
                "        print(f\"❌ Failed Strategy C: {e}\")\n",
                "        raise RuntimeError(\"Could not load ANY feature extraction model. Check internet/installation.\")\n",
                "\n",
                "print(f\"\\nFINAL MODEL SELECTION: {model_name} (Dim: {EMBEDDING_DIM})\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_audio_fixed(file_path):\n",
                "    \"\"\"Load audio and ensure it's exactly 32000 samples (2s @ 16kHz)\"\"\"\n",
                "    audio, _ = librosa.load(str(file_path), sr=TARGET_SR, mono=True)\n",
                "    if len(audio) < N_SAMPLES:\n",
                "        audio = np.pad(audio, (0, N_SAMPLES - len(audio)))\n",
                "    return audio[:N_SAMPLES].astype(np.float32)\n",
                "\n",
                "def extract_embeddings_batch(audio_batch):\n",
                "    \"\"\"Extract embeddings correctly for the selected model\"\"\"\n",
                "    audio_array = np.array(audio_batch, dtype=np.float32)\n",
                "    \n",
                "    # Logic for HeAR Transformers\n",
                "    if model_name == \"HeAR_Transformers\":\n",
                "        audio_tensor = tf.convert_to_tensor(audio_array)\n",
                "        # Transformers model often callable directly\n",
                "        out = embedding_model(audio_tensor)\n",
                "        # Use pooler_output if available, else mean of last_hidden_state\n",
                "        if hasattr(out, 'pooler_output'):\n",
                "            return out.pooler_output.numpy()\n",
                "        if hasattr(out, 'last_hidden_state'):\n",
                "            return tf.reduce_mean(out.last_hidden_state, axis=1).numpy()\n",
                "        return out[0].numpy()\n",
                "\n",
                "    # Logic for HeAR Native\n",
                "    elif \"HeAR\" in model_name:\n",
                "        if hasattr(embedding_model, 'signatures') and 'serving_default' in embedding_model.signatures:\n",
                "            fn = embedding_model.signatures['serving_default']\n",
                "            res = fn(tf.convert_to_tensor(audio_array))\n",
                "            return list(res.values())[0].numpy()\n",
                "        else:\n",
                "            return embedding_model(audio_array).numpy()\n",
                "\n",
                "    # Logic for YAMNet\n",
                "    elif model_name == \"YAMNet\":\n",
                "        # YAMNet does not support native batching of waveforms in TFHub version usually\n",
                "        # It expects 1D tensor. We loop.\n",
                "        batch_embs = []\n",
                "        for waveform in audio_array:\n",
                "            _, embeddings, _ = embedding_model(waveform)\n",
                "            # embeddings shape: (N, 1024). Average them.\n",
                "            avg_emb = tf.reduce_mean(embeddings, axis=0)\n",
                "            batch_embs.append(avg_emb)\n",
                "        return np.array(batch_embs)\n",
                "\n",
                "    return np.zeros((len(audio_batch), EMBEDDING_DIM))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "def process_dataset_embeddings(dataset_name, batch_size=32):\n",
                "    \"\"\"Process all audio files from a dataset and extract embeddings\"\"\"\n",
                "    input_dir = PROCESSED_ROOT / dataset_name\n",
                "    if not input_dir.exists():\n",
                "        print(f\"⚠ {dataset_name}: Not found at {input_dir}\")\n",
                "        return None\n",
                "\n",
                "    wav_files = sorted(input_dir.glob(\"*.wav\"))\n",
                "    if not wav_files:\n",
                "        print(f\"⚠ {dataset_name}: No WAV files in {input_dir}\")\n",
                "        return None\n",
                "\n",
                "    print(f\"\\nProcessing {dataset_name}: {len(wav_files)} files\")\n",
                "\n",
                "    embeddings_list = []\n",
                "    file_names = []\n",
                "\n",
                "    # Adjust batch size for YAMNet since we loop internally anyway, keep it managed\n",
                "    for i in tqdm(range(0, len(wav_files), batch_size), desc=f\"Extracting {dataset_name}\"):\n",
                "        batch_files = wav_files[i:i+batch_size]\n",
                "        batch_audio = [load_audio_fixed(f) for f in batch_files]\n",
                "        try:\n",
                "            batch_embeddings = extract_embeddings_batch(batch_audio)\n",
                "            embeddings_list.append(batch_embeddings)\n",
                "            file_names.extend([f.stem for f in batch_files])\n",
                "        except Exception as e:\n",
                "            print(f\"Error processing batch starting at {i}: {e}\")\n",
                "            continue\n",
                "\n",
                "    if not embeddings_list:\n",
                "        return 0\n",
                "\n",
                "    embeddings = np.vstack(embeddings_list)\n",
                "\n",
                "    output_path = EMBEDDINGS_DIR / f\"{dataset_name}_embeddings.npz\"\n",
                "    np.savez_compressed(output_path, embeddings=embeddings, file_names=file_names)\n",
                "\n",
                "    print(f\"✓ {dataset_name}: {embeddings.shape[0]} embeddings saved\")\n",
                "    return embeddings.shape[0]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Processing coughvid: 11319 files\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "4f4084dbe6dc4e9c904f7c2e4949b9ce",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Extracting coughvid:   0%|          | 0/354 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "✓ coughvid: 11319 embeddings saved\n",
                        "\n",
                        "Processing parkinsons: 5 files\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "5ff25395ca974396925d72e583b6ea24",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Extracting parkinsons:   0%|          | 0/1 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "✓ parkinsons: 5 embeddings saved\n",
                        "\n",
                        "Processing respiratory_sounds: 9841 files\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "9710e90be1eb4714bdf3d48d2c1a48b4",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Extracting respiratory_sounds:   0%|          | 0/308 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "✓ respiratory_sounds: 9841 embeddings saved\n",
                        "\n",
                        "Processing coswara: 5 files\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "5e519c5fdd3c4f76aca74c9b2f411d7c",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Extracting coswara:   0%|          | 0/1 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "✓ coswara: 5 embeddings saved\n",
                        "\n",
                        "==================================================\n",
                        "Total embeddings: 21170\n",
                        "Output: D:\\datasets\\embeddings\n",
                        "\n",
                        "Proceed to: C_multi_task_classifier.ipynb\n"
                    ]
                }
            ],
            "source": [
                "# Process all datasets\n",
                "datasets = ['coughvid', 'parkinsons', 'respiratory_sounds', 'coswara']\n",
                "results = {}\n",
                "\n",
                "for name in datasets:\n",
                "    results[name] = process_dataset_embeddings(name)\n",
                "\n",
                "# Save summary\n",
                "summary = {\n",
                "    'embedding_dim': EMBEDDING_DIM, \n",
                "    'sample_rate': TARGET_SR, \n",
                "    'model_name': model_name,\n",
                "    'datasets': results\n",
                "}\n",
                "with open(EMBEDDINGS_DIR / 'embeddings_summary.json', 'w') as f:\n",
                "    json.dump(summary, f, indent=2)\n",
                "\n",
                "print(f\"\\n\" + \"=\"*50)\n",
                "total = sum(v for v in results.values() if v)\n",
                "print(f\"Total embeddings: {total}\")\n",
                "print(f\"Output: {EMBEDDINGS_DIR}\")\n",
                "print(\"\\nProceed to: C_multi_task_classifier.ipynb\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
