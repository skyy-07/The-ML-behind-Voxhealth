{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Notebook C: Multi-Task Disease Classifier\n",
                "## Training separate heads for TB, Parkinson's, and Pulmonary anomalies\n",
                "\n",
                "Uses HeAR embeddings to train classifiers for:\n",
                "- **TB Detection**: Coughvid + Coswara datasets\n",
                "- **Parkinson's Detection**: Voice dataset\n",
                "- **Pulmonary Anomalies**: Respiratory Sound Database"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Embeddings: D:\\datasets\\embeddings\n",
                        "Models output: D:\\datasets\\models\n"
                    ]
                }
            ],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "from pathlib import Path\n",
                "import json\n",
                "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
                "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
                "from sklearn.neural_network import MLPClassifier\n",
                "from sklearn.metrics import classification_report, roc_auc_score, f1_score\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "DATASETS_ROOT = Path(r\"D:\\datasets\")\n",
                "EMBEDDINGS_DIR = DATASETS_ROOT / 'embeddings'\n",
                "MODELS_DIR = DATASETS_ROOT / 'models'\n",
                "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "print(f\"Embeddings: {EMBEDDINGS_DIR}\")\n",
                "print(f\"Models output: {MODELS_DIR}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "✓ coughvid: (11319, 1024)\n",
                        "✓ parkinsons: (5, 1024)\n",
                        "✓ respiratory_sounds: (9841, 1024)\n",
                        "✓ coswara: (5, 1024)\n"
                    ]
                }
            ],
            "source": [
                "def load_embeddings(dataset_name):\n",
                "    \"\"\"Load embeddings for a dataset\"\"\"\n",
                "    path = EMBEDDINGS_DIR / f\"{dataset_name}_embeddings.npz\"\n",
                "    if not path.exists():\n",
                "        print(f\"⚠ {dataset_name}: Embeddings not found\")\n",
                "        return None, None\n",
                "    data = np.load(path, allow_pickle=True)\n",
                "    return data['embeddings'], data['file_names']\n",
                "\n",
                "# Load all embeddings\n",
                "embeddings = {}\n",
                "file_names = {}\n",
                "for name in ['coughvid', 'parkinsons', 'respiratory_sounds', 'coswara']:\n",
                "    emb, files = load_embeddings(name)\n",
                "    if emb is not None:\n",
                "        embeddings[name] = emb\n",
                "        file_names[name] = files\n",
                "        print(f\"✓ {name}: {emb.shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Label loading functions defined\n"
                    ]
                }
            ],
            "source": [
                "def load_labels_coughvid():\n",
                "    \"\"\"Load labels from Coughvid metadata (COVID status)\"\"\"\n",
                "    csv_path = DATASETS_ROOT / 'coughvid' / 'metadata_compiled.csv'\n",
                "    if csv_path.exists():\n",
                "        df = pd.read_csv(csv_path)\n",
                "        # Map status to binary (COVID positive vs negative)\n",
                "        return df.set_index('uuid')['status'].to_dict()\n",
                "    return {}\n",
                "\n",
                "def load_labels_coswara():\n",
                "    \"\"\"Load labels from Coswara metadata (COVID status)\"\"\"\n",
                "    csv_path = DATASETS_ROOT / 'coswara' / 'combined_data.csv'\n",
                "    if csv_path.exists():\n",
                "        df = pd.read_csv(csv_path)\n",
                "        return df.set_index('id')['covid_status'].to_dict()\n",
                "    return {}\n",
                "\n",
                "def load_labels_respiratory():\n",
                "    \"\"\"Load labels from Respiratory database (diagnosis)\"\"\"\n",
                "    csv_path = DATASETS_ROOT / 'respiratory_sounds' / 'Respiratory_Sound_Database'\n",
                "    for p in [csv_path / 'patient_diagnosis.csv', DATASETS_ROOT / 'respiratory_sounds' / 'patient_diagnosis.csv']:\n",
                "        if p.exists():\n",
                "            df = pd.read_csv(p)\n",
                "            return df.set_index('Patient number')['Diagnosis'].to_dict()\n",
                "    return {}\n",
                "\n",
                "print(\"Label loading functions defined\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "✓ DiseaseClassifier defined\n"
                    ]
                }
            ],
            "source": [
                "class DiseaseClassifier:\n",
                "    \"\"\"Multi-head classifier for respiratory diseases\"\"\"\n",
                "    \n",
                "    def __init__(self, model_type='mlp'):\n",
                "        self.model_type = model_type\n",
                "        self.scaler = StandardScaler()\n",
                "        self.label_encoder = LabelEncoder()\n",
                "        self._init_model()\n",
                "    \n",
                "    def _init_model(self):\n",
                "        if self.model_type == 'mlp':\n",
                "            self.model = MLPClassifier(hidden_layer_sizes=(256, 128, 64), max_iter=500, early_stopping=True)\n",
                "        elif self.model_type == 'rf':\n",
                "            self.model = RandomForestClassifier(n_estimators=100, max_depth=10)\n",
                "        elif self.model_type == 'gb':\n",
                "            self.model = GradientBoostingClassifier(n_estimators=100, max_depth=5)\n",
                "        else:\n",
                "            self.model = LogisticRegression(max_iter=1000, C=0.1)\n",
                "    \n",
                "    def fit(self, X, y):\n",
                "        X_scaled = self.scaler.fit_transform(X)\n",
                "        y_encoded = self.label_encoder.fit_transform(y)\n",
                "        self.model.fit(X_scaled, y_encoded)\n",
                "        return self\n",
                "    \n",
                "    def predict(self, X):\n",
                "        X_scaled = self.scaler.transform(X)\n",
                "        y_pred = self.model.predict(X_scaled)\n",
                "        return self.label_encoder.inverse_transform(y_pred)\n",
                "    \n",
                "    def predict_proba(self, X):\n",
                "        X_scaled = self.scaler.transform(X)\n",
                "        return self.model.predict_proba(X_scaled)\n",
                "    \n",
                "    def evaluate(self, X, y):\n",
                "        y_pred = self.predict(X)\n",
                "        y_encoded = self.label_encoder.transform(y)\n",
                "        y_pred_encoded = self.label_encoder.transform(y_pred)\n",
                "        \n",
                "        f1 = f1_score(y_encoded, y_pred_encoded, average='weighted')\n",
                "        try:\n",
                "            y_proba = self.predict_proba(X)\n",
                "            if len(self.label_encoder.classes_) == 2:\n",
                "                auc = roc_auc_score(y_encoded, y_proba[:, 1])\n",
                "            else:\n",
                "                auc = roc_auc_score(y_encoded, y_proba, multi_class='ovr', average='weighted')\n",
                "        except:\n",
                "            auc = None\n",
                "        \n",
                "        return {'f1': f1, 'auc': auc, 'report': classification_report(y, y_pred)}\n",
                "\n",
                "print(\"✓ DiseaseClassifier defined\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "==================================================\n",
                        "TRAINING TB/COVID CLASSIFIER\n",
                        "==================================================\n",
                        "Using coughvid embeddings: (11319, 1024)\n",
                        "Using coswara embeddings: (5, 1024)\n",
                        "F1 Score: 0.5041\n",
                        "AUC-ROC: 0.5167\n"
                    ]
                }
            ],
            "source": [
                "def train_tb_classifier():\n",
                "    \"\"\"Train TB/COVID classifier using Coughvid + Coswara\"\"\"\n",
                "    print(\"\\n\" + \"=\"*50)\n",
                "    print(\"TRAINING TB/COVID CLASSIFIER\")\n",
                "    print(\"=\"*50)\n",
                "    \n",
                "    # Combine embeddings from both datasets\n",
                "    X_list, y_list = [], []\n",
                "    \n",
                "    for dataset in ['coughvid', 'coswara']:\n",
                "        if dataset in embeddings:\n",
                "            print(f\"Using {dataset} embeddings: {embeddings[dataset].shape}\")\n",
                "            # For demo: create synthetic labels (0=healthy, 1=positive)\n",
                "            n = len(embeddings[dataset])\n",
                "            X_list.append(embeddings[dataset])\n",
                "            y_list.append(np.random.randint(0, 2, n))  # Replace with actual labels\n",
                "    \n",
                "    if not X_list:\n",
                "        print(\"No data available\")\n",
                "        return None\n",
                "    \n",
                "    X = np.vstack(X_list)\n",
                "    y = np.concatenate(y_list)\n",
                "    \n",
                "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n",
                "    \n",
                "    clf = DiseaseClassifier('mlp')\n",
                "    clf.fit(X_train, y_train)\n",
                "    results = clf.evaluate(X_test, y_test)\n",
                "    \n",
                "    print(f\"F1 Score: {results['f1']:.4f}\")\n",
                "    print(f\"AUC-ROC: {results['auc']:.4f}\" if results['auc'] else \"AUC: N/A\")\n",
                "    \n",
                "    return clf, results\n",
                "\n",
                "tb_clf, tb_results = train_tb_classifier()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "==================================================\n",
                        "TRAINING PARKINSON'S CLASSIFIER\n",
                        "==================================================\n"
                    ]
                },
                {
                    "ename": "ValueError",
                    "evalue": "The test_size = 1 should be greater or equal to the number of classes = 2",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     21\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAUC-ROC: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults[\u001b[33m'\u001b[39m\u001b[33mauc\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m results[\u001b[33m'\u001b[39m\u001b[33mauc\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mAUC: N/A\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     23\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m clf, results\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m pd_clf, pd_results = \u001b[43mtrain_parkinsons_classifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mtrain_parkinsons_classifier\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     11\u001b[39m X = embeddings[\u001b[33m'\u001b[39m\u001b[33mparkinsons\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     12\u001b[39m y = np.random.randint(\u001b[32m0\u001b[39m, \u001b[32m2\u001b[39m, \u001b[38;5;28mlen\u001b[39m(X))  \u001b[38;5;66;03m# Replace with actual labels\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m X_train, X_test, y_train, y_test = \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstratify\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m clf = DiseaseClassifier(\u001b[33m'\u001b[39m\u001b[33mmlp\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     17\u001b[39m clf.fit(X_train, y_train)\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\utils\\_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\model_selection\\_split.py:2940\u001b[39m, in \u001b[36mtrain_test_split\u001b[39m\u001b[34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[39m\n\u001b[32m   2936\u001b[39m         CVClass = ShuffleSplit\n\u001b[32m   2938\u001b[39m     cv = CVClass(test_size=n_test, train_size=n_train, random_state=random_state)\n\u001b[32m-> \u001b[39m\u001b[32m2940\u001b[39m     train, test = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstratify\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2942\u001b[39m train, test = ensure_common_namespace_device(arrays[\u001b[32m0\u001b[39m], train, test)\n\u001b[32m   2944\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\n\u001b[32m   2945\u001b[39m     chain.from_iterable(\n\u001b[32m   2946\u001b[39m         (_safe_indexing(a, train), _safe_indexing(a, test)) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m arrays\n\u001b[32m   2947\u001b[39m     )\n\u001b[32m   2948\u001b[39m )\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\model_selection\\_split.py:1927\u001b[39m, in \u001b[36mBaseShuffleSplit.split\u001b[39m\u001b[34m(self, X, y, groups)\u001b[39m\n\u001b[32m   1897\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Generate indices to split data into training and test set.\u001b[39;00m\n\u001b[32m   1898\u001b[39m \n\u001b[32m   1899\u001b[39m \u001b[33;03mParameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1924\u001b[39m \u001b[33;03mto an integer.\u001b[39;00m\n\u001b[32m   1925\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1926\u001b[39m X, y, groups = indexable(X, y, groups)\n\u001b[32m-> \u001b[39m\u001b[32m1927\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_iter_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1928\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\model_selection\\_split.py:2355\u001b[39m, in \u001b[36mStratifiedShuffleSplit._iter_indices\u001b[39m\u001b[34m(self, X, y, groups)\u001b[39m\n\u001b[32m   2350\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2351\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe train_size = \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m should be greater or \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2352\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mequal to the number of classes = \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m\"\u001b[39m % (n_train, n_classes)\n\u001b[32m   2353\u001b[39m     )\n\u001b[32m   2354\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_test < n_classes:\n\u001b[32m-> \u001b[39m\u001b[32m2355\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2356\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe test_size = \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m should be greater or \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2357\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mequal to the number of classes = \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m\"\u001b[39m % (n_test, n_classes)\n\u001b[32m   2358\u001b[39m     )\n\u001b[32m   2360\u001b[39m \u001b[38;5;66;03m# Find the sorted list of instances for each class:\u001b[39;00m\n\u001b[32m   2361\u001b[39m \u001b[38;5;66;03m# (np.unique above performs a sort, so code is O(n logn) already)\u001b[39;00m\n\u001b[32m   2362\u001b[39m class_indices = np.split(\n\u001b[32m   2363\u001b[39m     np.argsort(y_indices, kind=\u001b[33m\"\u001b[39m\u001b[33mmergesort\u001b[39m\u001b[33m\"\u001b[39m), np.cumsum(class_counts)[:-\u001b[32m1\u001b[39m]\n\u001b[32m   2364\u001b[39m )\n",
                        "\u001b[31mValueError\u001b[39m: The test_size = 1 should be greater or equal to the number of classes = 2"
                    ]
                }
            ],
            "source": [
                "def train_parkinsons_classifier():\n",
                "    \"\"\"Train Parkinson's classifier\"\"\"\n",
                "    print(\"\\n\" + \"=\"*50)\n",
                "    print(\"TRAINING PARKINSON'S CLASSIFIER\")\n",
                "    print(\"=\"*50)\n",
                "    \n",
                "    if 'parkinsons' not in embeddings:\n",
                "        print(\"Parkinson's embeddings not available\")\n",
                "        return None, None\n",
                "    \n",
                "    X = embeddings['parkinsons']\n",
                "    y = np.random.randint(0, 2, len(X))  # Replace with actual labels\n",
                "    \n",
                "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n",
                "    \n",
                "    clf = DiseaseClassifier('mlp')\n",
                "    clf.fit(X_train, y_train)\n",
                "    results = clf.evaluate(X_test, y_test)\n",
                "    \n",
                "    print(f\"F1 Score: {results['f1']:.4f}\")\n",
                "    print(f\"AUC-ROC: {results['auc']:.4f}\" if results['auc'] else \"AUC: N/A\")\n",
                "    \n",
                "    return clf, results\n",
                "\n",
                "pd_clf, pd_results = train_parkinsons_classifier()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "==================================================\n",
                        "TRAINING PULMONARY ANOMALY CLASSIFIER\n",
                        "==================================================\n",
                        "F1 Score: 0.2589\n",
                        "AUC-ROC: 0.5020\n"
                    ]
                }
            ],
            "source": [
                "def train_pulmonary_classifier():\n",
                "    \"\"\"Train Pulmonary anomaly classifier\"\"\"\n",
                "    print(\"\\n\" + \"=\"*50)\n",
                "    print(\"TRAINING PULMONARY ANOMALY CLASSIFIER\")\n",
                "    print(\"=\"*50)\n",
                "    \n",
                "    if 'respiratory_sounds' not in embeddings:\n",
                "        print(\"Respiratory sounds embeddings not available\")\n",
                "        return None, None\n",
                "    \n",
                "    X = embeddings['respiratory_sounds']\n",
                "    # Labels: 0=normal, 1=crackles, 2=wheezes, 3=both\n",
                "    y = np.random.randint(0, 4, len(X))  # Replace with actual labels\n",
                "    \n",
                "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n",
                "    \n",
                "    clf = DiseaseClassifier('mlp')\n",
                "    clf.fit(X_train, y_train)\n",
                "    results = clf.evaluate(X_test, y_test)\n",
                "    \n",
                "    print(f\"F1 Score: {results['f1']:.4f}\")\n",
                "    print(f\"AUC-ROC: {results['auc']:.4f}\" if results['auc'] else \"AUC: N/A\")\n",
                "    \n",
                "    return clf, results\n",
                "\n",
                "pulm_clf, pulm_results = train_pulmonary_classifier()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "ename": "NameError",
                    "evalue": "name 'pd_clf' is not defined",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Save models\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpickle\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m models = {\u001b[33m'\u001b[39m\u001b[33mtb\u001b[39m\u001b[33m'\u001b[39m: tb_clf, \u001b[33m'\u001b[39m\u001b[33mparkinsons\u001b[39m\u001b[33m'\u001b[39m: \u001b[43mpd_clf\u001b[49m, \u001b[33m'\u001b[39m\u001b[33mpulmonary\u001b[39m\u001b[33m'\u001b[39m: pulm_clf}\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, clf \u001b[38;5;129;01min\u001b[39;00m models.items():\n\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m clf:\n",
                        "\u001b[31mNameError\u001b[39m: name 'pd_clf' is not defined"
                    ]
                }
            ],
            "source": [
                "# Save models\n",
                "import pickle\n",
                "\n",
                "models = {'tb': tb_clf, 'parkinsons': pd_clf, 'pulmonary': pulm_clf}\n",
                "for name, clf in models.items():\n",
                "    if clf:\n",
                "        with open(MODELS_DIR / f\"{name}_classifier.pkl\", 'wb') as f:\n",
                "            pickle.dump(clf, f)\n",
                "        print(f\"✓ Saved {name} classifier\")\n",
                "\n",
                "# Save results summary\n",
                "summary = {\n",
                "    'tb': {'f1': tb_results['f1'], 'auc': tb_results['auc']} if tb_results else None,\n",
                "    'parkinsons': {'f1': pd_results['f1'], 'auc': pd_results['auc']} if pd_results else None,\n",
                "    'pulmonary': {'f1': pulm_results['f1'], 'auc': pulm_results['auc']} if pulm_results else None\n",
                "}\n",
                "with open(MODELS_DIR / 'training_summary.json', 'w') as f:\n",
                "    json.dump(summary, f, indent=2)\n",
                "\n",
                "print(f\"\\nModels saved to: {MODELS_DIR}\")\n",
                "print(\"\\nRun validation.py for cross-validation evaluation\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
